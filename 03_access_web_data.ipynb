{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous method\n",
    "# hand = open('data/mbox-short.txt')\n",
    "# for line in hand:\n",
    "#     line = line.rstrip()\n",
    "#     if line.find('From:') >= 0:\n",
    "#         print(line)\n",
    "\n",
    "# Regex method\n",
    "import re\n",
    "\n",
    "hand = open('data/mbox-short.txt')\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    if re.search('From:', line):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching and extracting data\n",
    "x = 'My 2 favourite numbers are 19 and 42'\n",
    "print(re.findall('[1-9]+', x))\n",
    "print(re.findall('[a-h]', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Greedy matching\n",
    "x = 'From: Using the : character'\n",
    "print(re.findall('^F.+:', x)) # F = From, .+ = one or more characters, : = colon\n",
    "# Non-greedy matching\n",
    "print(re.findall('^F.+?:', x)) # F = From, .+ = one or more characters, ? = non-greedy, : = colon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning string extraction\n",
    "x = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'\n",
    "print(re.findall(r'\\S+@\\S+', x)) # \\S = non-whitespace, + = one or more characters\n",
    "print(re.findall(r'^From (\\S+@\\S+)', x)) # ^ = start of line, () = extract only the email address\n",
    "print(re.findall(r'^From .*@([^ ]*)', x)) # .* = any character, zero or more times, () = extract only the domain name\n",
    "print(re.findall(r'\\S+?@\\S+', x)) # .* = any character, zero or more times, () = extract only the domain name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand = open('data/mbox-short.txt')\n",
    "numlist = list()\n",
    "for line in hand:\n",
    "    line = line.rstrip()\n",
    "    stuff = re.findall('^X-DSPAM-Confidence: ([0-9.]+)', line)\n",
    "    if len(stuff) != 1: continue\n",
    "    num = float(stuff[0])\n",
    "    numlist.append(num)\n",
    "print('Maximum:', max(numlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex assignment\n",
    "import re\n",
    "text = open('data/regex_assignment.txt')\n",
    "numlist = list()\n",
    "for line in text:\n",
    "    # line = line.rstrip()\n",
    "    numbers = re.findall('[0-9]+', line)\n",
    "    for number in numbers:\n",
    "        num = int(number)\n",
    "        numlist.append(num)\n",
    "print(sum(numlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks and sockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode() # encode() converts string to bytes\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), # decode() converts bytes to string (Unicode)\n",
    "        end=''\n",
    "    )\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET /intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode()) # decode() converts bytes to string\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http:// is the protocol\n",
    "# www.dr-chuck.com is the host\n",
    "# /page1.html is the document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-life example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of connecting to an API and extracting data from it\n",
    "\n",
    "import websocket\n",
    "import json\n",
    "\n",
    "# Función que recibe y muestra los datos\n",
    "def on_message(ws, message):\n",
    "    data = json.loads(message)\n",
    "    price = data.get('p', 'No data')  # Extrae el precio\n",
    "    print(f'Precio de Bitcoin: {price} USD')\n",
    "\n",
    "# Conectarse a la API de Binance (precios en tiempo real)\n",
    "url = 'wss://stream.binance.com:9443/ws/btcusdt@trade'\n",
    "\n",
    "# Crear conexión WebSocket\n",
    "ws = websocket.WebSocketApp(url, on_message=on_message)\n",
    "# ws.run_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programs that surf the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASCII Mapping vs UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each character is represented by a number between 0 and 256 stored \n",
    "# in 8 bits of memory. ord() function returns the number representing \n",
    "# the unicode code of a specified character\n",
    "print(ord('H'))\n",
    "print(ord('e'))\n",
    "print(ord('\\n'))\n",
    "\n",
    "# chr() function returns the character that represents the specified\n",
    "# unicode code\n",
    "print(chr(72))\n",
    "print(chr(101))\n",
    "print(chr(10))\n",
    "print(chr(108), chr(105), chr(110), chr(101))\n",
    "\n",
    "# For Python 3, strings are Unicode by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to understand the differences between the tipes of character mapping when we talk to a network, because their characters might be in a different mapping than then one we are talking to. That's why we need to encode and decode. However, the big majority (around 99%) of characters there are in the web are already UTF-8, making this a lot eaasier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "x = b'abc'\n",
    "print(type(x))\n",
    "x = u'abc'\n",
    "print(type(x))\n",
    "x = 'abc'\n",
    "print(type(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode() # encode() converts string to bytes\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512) # Data is received as bytes\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    mystring = data.decode() # decode() turns the character set to UTF-8 or ASCII by default\n",
    "    print(mystring) # Data is now Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using urllib in Python\n",
    "Since HTTP is so common, we have a libray that does all the socket work for us and makes web pages look like a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "# Even though this doesn't return the headers, it reads them, and you can ask for them later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To count the most repetitive words in a web file\n",
    "counts = dict()\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with another link\n",
    "fhand = urllib.request.urlopen('http://www.dr-chuck.com/page1.htm')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: requests library + BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: `import requests` is a better practice to manage all this things regarding talking to the web with Python as it is a high level librarie that automates many of the manual steps done with sockets or urllib libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://es.wikipedia.org/wiki/Wikipedia:Portada'\n",
    "\n",
    "# Send a GET request and get the response text\n",
    "response = requests.get(url)\n",
    "print(response.text) # Can also use .status_code, .headers, .json(), .content, .url, .encoding, .apparent_encoding, .history, .cookies, .elapsed, .request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use BeautifulSoup to scrape information from a HTML web page that has issues with its syntax (sytax errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "html_doc = '<html><body><h1>Hello, World!</h1></body></html>'\n",
    "\n",
    "# Parsing the HTML\n",
    "soup = bs(html_doc, 'html.parser') # html.parser is the parser\n",
    "\n",
    "print(soup.text) # Extracting the text from the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://es.wikipedia.org/wiki/Wikipedia:Portada'\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Print the page title\n",
    "    print('Page Title:', soup.title.string)\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')  # Finds all <a> (anchor) tags\n",
    "\n",
    "    # Print the first 5 links\n",
    "    for link in links[:5]:\n",
    "        print(link.get('href'))  # Get the 'href' attribute (URL)\n",
    "else:\n",
    "    print('Failed to fetch the page:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to BeautifulSoup and urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter url: ')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None)) # get() method returns the value of the 'href' attribute, or None if the attribute is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example code using urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the version of ssl installed\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl \n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = ('http://www.dr-chuck.com/page1.htm')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None)) # get() method returns the value of the 'href' attribute, or None if\n",
    "    # the attribute is not present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same example code using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.dr-chuck.com/page1.htm'  # Replace with the actual URL\n",
    "\n",
    "# Send a GET request (disabling SSL verification like in urllib)\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Retrieve all anchor tags\n",
    "tags = soup.find_all('a')  # Finds all <a> elements\n",
    "\n",
    "# Print their href attributes\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))  # get() returns None if 'href' doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading an internet table and adding up the numbers\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://py4e-data.dr-chuck.net/comments_2144910.html'\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all the numbers inside <span class=\"comments\">\n",
    "numbers = [int(tag.text) for tag in soup.find_all('span', class_='comments')]\n",
    "\n",
    "total = sum(numbers)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Following links multiple times until I retrieve the 18th link\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = 'https://py4e-data.dr-chuck.net/known_by_Sambrid.html'\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for i in range(7):\n",
    "    tags = soup('a')\n",
    "    link = tags[17].get('href', None)\n",
    "    html = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "print(re.findall('known_by_(.+).html', link)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
